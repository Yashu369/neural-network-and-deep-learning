import numpy as np

# ---------------------------
# Utility functions
# ---------------------------

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(a):
    return a * (1 - a)

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(float)

# ---------------------------
# Neural network class
# ---------------------------

class SimpleNN:
    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):
        # Initialize weights
        self.W1 = np.random.randn(hidden_dim, input_dim) * 0.01
        self.b1 = np.zeros((hidden_dim, 1))
        self.W2 = np.random.randn(output_dim, hidden_dim) * 0.01
        self.b2 = np.zeros((output_dim, 1))
        self.lr = lr

    def forward(self, X):
        # X shape: (features, samples)
        self.Z1 = self.W1 @ X + self.b1
        self.A1 = relu(self.Z1)
        self.Z2 = self.W2 @ self.A1 + self.b2
        self.A2 = sigmoid(self.Z2)
        return self.A2

    def backward(self, X, Y):
        m = X.shape[1]

        # Output layer gradients
        dZ2 = self.A2 - Y
        dW2 = (1/m) * (dZ2 @ self.A1.T)
        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)

        # Hidden layer gradients
        dA1 = self.W2.T @ dZ2
        dZ1 = dA1 * relu_derivative(self.Z1)
        dW1 = (1/m) * (dZ1 @ X.T)
        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)

        # Update parameters
        self.W2 -= self.lr * dW2
        self.b2 -= self.lr * db2
        self.W1 -= self.lr * dW1
        self.b1 -= self.lr * db1

    def train(self, X, Y, epochs=1000):
        for epoch in range(epochs):
            A2 = self.forward(X)
            self.backward(X, Y)

            if epoch % 100 == 0:
                loss = -np.mean(Y*np.log(A2+1e-8) + (1-Y)*np.log(1-A2+1e-8))
                print(f"Epoch {epoch}, Loss = {loss:.4f}")

# ---------------------------
# Example usage
# ---------------------------

# Dummy dataset (XOR)
X = np.array([[0,0,1,1],
              [0,1,0,1]])  # (2 features, 4 samples)

Y = np.array([[0,1,1,0]])  # XOR labels

nn = SimpleNN(input_dim=2, hidden_dim=4, output_dim=1, lr=0.1)
nn.train(X, Y, epochs=2000)

# Make predictions
preds = nn.forward(X)
print("Predictions:", preds)
